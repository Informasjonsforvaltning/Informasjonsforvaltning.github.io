[{"uri":"/developers_handbook/tools_and_software/container/","title":"Container","tags":[],"description":"","content":"Container Engine We recommend using Podman as your container engine. Podman is rootless by design, which makes it a more secure solution. Its daemonless design removes the single point of failure created by Docker\u0026rsquo;s central daemon.\nRead more about Podman\nInstallation (Podman Desktop) brew install podman Setup Podman container engine On macOS and Windows, running the Podman container engine requires running a Linux virtual machine.\nBy default, Podman Desktop initializes a Podman machine with a standard configuration.\nConsider creating a custom Podman machine to:\nControl the assigned resources: CPUs, memory, and disk size. Use a custom boot image. Use the rootful connection by default, for example to run Kind. (On Windows) Route the traffic through the network connection from your Windows session. Prerequisites​\nThe Podman executable is installed. Procedure​\nGo to Settings \u0026gt; Resources. In the Podman tile, click Create new. In the Create a Podman machine screen: Name: Enter a name, such as podman-machine-default. CPU(s): Select the number of CPUs, e.g. 2. Memory: Select the memory size, e.g. 2048 MiB Disk size: Select the disk size, e.g. 20 GiB. Image path (Optional): Select a bootable image containing a virtual machine with Podman. Machine with root privileges: Enable to use the rootful connection by default. Required to use Kind on Windows. (On Windows) User mode networking (traffic relayed by a user process): - Enable to route the traffic through the network connection from your - Windows session. Required to access resources behind your VPN connection. Click Create Docker compose Docker compose has more features than Podman compose and therefor the recommended tool for running compose files.\nInstall docker compose by executing the following:\nbrew install docker-compose Podman compose will now using /opt/homebrew/bin/docker-compose instead.\nLinking Docker Some test frameworks call the docker command. Becaus To resolve this we have to make a static link to the podman command. Find the path to your podman binary:\nwhich podman and then replace the below with your path and run the command:\nsudo ln -s \u0026lt;path-to-your-podman-binary\u0026gt; /usr/local/bin/docker "},{"uri":"/developers_handbook/code_quality/coverage/","title":"Coverage","tags":[],"description":"","content":"Test coverage numbers are not the goal in itself. What is important to monitor is whether or not a pull request increases or decreases coverage. In most all cases a pull request should maintain or increase coverage. In extreme cases (e.g. when fixing a critical bug), it might be necessary to temporarily sacrifice coverage.\n"},{"uri":"/data.norge.no/","title":"Data.norge.no","tags":[],"description":"","content":"Data.norge.no is a solution that gives an overview of datasets, concepts, APIs, information models, services and events (and relations between these) in the public sector.\nHow can I test Data.norge.no? This is our demo environment: https://demo.fellesdatakatalog.digdir.no\nThis is our solution for registering resources in the demo environment: https://registrering.demo.fellesdatakatalog.digdir.no\nTo get a demo-user you can contect us at this email: fellesdatakatalog@digdir.no\nWhere can I read more? About the national data catalog About the search API "},{"uri":"/developers_handbook/microservices/","title":"Microservices","tags":[],"description":"","content":"We base our understanding of what a microservice is on the following definitions:\nMicroservices are small, autonomous services that work together.\nServices are out-of-process components who communicate with a mechanism such as a web service request, or remote procedure call.\nA component is a unit of software that is independently replaceable and upgradeable.\nReferences Sam Newman, Building Microservices\nMartin Fowler, Microservices\n"},{"uri":"/data.norge.no/overordnet-arkitektur/n%C3%A5situasjon/","title":"Nåsituasjon","tags":[],"description":"","content":"Her følger en kort oversikt over den overordnede arkitekturen til våre komponenter.\nDiagram Sammenhengen mellom komponentene illustreres i følgende diagram: Om komponentene i nåsituasjon I dag har vi delt de ulike komponentene inn i en løsning for søk i kataloger, en løsning for registrering, en løsning for innhøsting (harvester). I tillegg har vi en intern komponent for referanse-data.\nSøkeløsningen search: Vår portal for søk på tvers av alle katalogene search-api: Eksternt api med oppslag mot datasettkatalogen. api-cat: Eksternt api med oppslag mot api-katakogen. concept-cat: Eksternt api med oppslag mot begreps-katalogen. Registreringsløsningen registration-react: Vårt brukergrensesnitt for å registrere og legge inn innhold i kataloger registration-api: backend-komponent for Registreringsløsningen registration-auth: komponent som håndterer innlogging via IDPorten Høsteløsningen Høsteløsningen laster ned datakataloger og gjør dem søkbare. Består av\nharvester harvester-api Modellene Archi-modellen: https://github.com/Informasjonsforvaltning/SA_Informasjonsforvaltning_BASELINE Html-versjon: https://informasjonsforvaltning.github.io/SA_Informasjonsforvaltning_BASELINE/ "},{"uri":"/data.norge.no/harvesting/rdf-parse/","title":"RDF parsing","tags":[],"description":"","content":"RDF parsing consists of three components:\nRDF Parser Parser Library written in Python Repository: fdk-rdf-parser\nRDF Parser Service JSON API service responsible for parsing RDF (turtle) to JSON using RDF Parser. Repository: fdk-rdf-parser-service\nRDF Parse Event Publisher Kafka consumer/producer consuming events of type reasoned and producing RDF Parse Events using RDF Parser Service. Repository: fdk-rdf-parse-event-publisher\n"},{"uri":"/data.norge.no/search/search/","title":"Search","tags":[],"description":"","content":"The service is responsible for handling searches is fdk-search-service, which is described by this OpenAPI specification.\nProduction endpoint: https://search.api.fellesdatakatalog.digdir.no/search Demo endpoint: https://search.api.demo.fellesdatakatalog.digdir.no/search Staging endpoint: https://search.api.staging.fellesdatakatalog.digdir.no/search\nSimple example using the staging endpoint:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;}\u0026#39; Searchable fields There are 3 searchable fields, they are title, description and keyword. The service will by default try to find matches for the query in all 3 fields, but it\u0026rsquo;s possible to define which of the fields it should include in the search body.\nExample where only hits from the description fields are included:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;, \u0026#34;fields\u0026#34;: {\u0026#34;title\u0026#34;:false,\u0026#34;description\u0026#34;:true,\u0026#34;keyword\u0026#34;:false}}\u0026#39; Boosting Hits from some fields will be prioritized over others, i.e. a matching hit from the title field will be prioritized over a hit from the description field.\nField Boost title, full phrase match 30 title, partial match 15 keyword 5 description 1 Take the title \u0026ldquo;Test search service\u0026rdquo; and the two queries \u0026ldquo;test service\u0026rdquo; and \u0026ldquo;search service\u0026rdquo;. The first query will have 2 partial matches \u0026ldquo;test\u0026rdquo; and \u0026ldquo;service\u0026rdquo;, with a combined search value of 15 + 15 = 30, the second query will have 3 matches where two are partial, \u0026ldquo;search\u0026rdquo; and \u0026ldquo;service\u0026rdquo;, and one is a full phrase match, \u0026ldquo;search service\u0026rdquo;, with a combined search value of 15 + 15 + 30 = 60.\nSpecific resource types Each resource type has it\u0026rsquo;s own endpoint, the available endpoints are /datasets, /data-services, /concepts, /information-models, /events and /services\nExample using the datasets endpoint:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search/datasets\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;}\u0026#39; Pagination All search results will be paginated, it is possible to customize the size and page number with the pagination field in the search body.\nExample using the pagination field, with current page set to number 5 and there are 10 hits per page:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;pagination\u0026#34;:{\u0026#34;size\u0026#34;:10,\u0026#34;page\u0026#34;:5}}\u0026#39; Filtering It\u0026rsquo;s possible to filter the search result, see SearchFilters in the OpenAPI specification for a list of all possible filters and what type of value they accept.\nExample using the data theme filter:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;filters\u0026#34;:{\u0026#34;dataTheme\u0026#34;:{\u0026#34;value\u0026#34;:[\u0026#34;ENVI\u0026#34;]}}}\u0026#39; Example using the open data filter:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;filters\u0026#34;:{\u0026#34;openData\u0026#34;:{\u0026#34;value\u0026#34;:true}}}\u0026#39; Example using the formats filter:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;filters\u0026#34;:{\u0026#34;formats\u0026#34;:{\u0026#34;value\u0026#34;:[\u0026#34;MEDIA_TYPE application/json\u0026#34;]}}}\u0026#39; Aggregations Each search result will include aggregations of the query for possible filter values. There are always included a value for each filter, it\u0026rsquo;s a list of the filter options represented in the total search result and a count of how many hits the filter option has.\nGiven that this search:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;}\u0026#39; Has this as the aggregation in the result:\n\u0026#34;aggregations\u0026#34;:{\u0026#34;accessRights\u0026#34;:[{\u0026#34;key\u0026#34;:\u0026#34;PUBLIC\u0026#34;,\u0026#34;count\u0026#34;:5},{\u0026#34;key\u0026#34;:\u0026#34;RESTRICTED\u0026#34;,\u0026#34;count\u0026#34;:16}]} Then the next example would therefore have 5 hits in it\u0026rsquo;s result, since the aggregation values shows that the query has 5 hits where the value for the access rights field is PUBLIC and 16 where the value is RESTRICTED.\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;filters\u0026#34;:{\u0026#34;accessRights\u0026#34;:{\u0026#34;value\u0026#34;:\u0026#34;PUBLIC\u0026#34;}}}\u0026#39; "},{"uri":"/developers_handbook/tools_and_software/frontend/","title":"Frontend","tags":[],"description":"","content":"Front-end tools Node.js\nWe use nvm to manage our node/npm versions. Yarn\nOur newer applications use Yarn, but older ones use npm to build and install the dependencies Node versions \u0026lt; 15 on mac To install node below 15 you need to do the following steps:\nOpen terminal in Rosetta2 mode: Go to Application -\u0026gt; Right click on terminal app -\u0026gt; Get Info -\u0026gt; Select \u0026ldquo;Open using Rosetta\u0026rdquo; -\u0026gt; Restart Terminal In Terminal, write -\u0026gt; arch -x86_64 zsh "},{"uri":"/data.norge.no/harvesting/harvesters/","title":"Harvesters","tags":[],"description":"","content":" fdk-concept-harvester fdk-dataservice-harvester fdk-dataset-harvester fdk-event-harvester fdk-informationmodel-harvester fdk-public-service-harvester The harvest process is triggered by messages from RabbitMQ with the routing key *.#.HarvestTrigger, where * is the relevant resource type, ie dataset, and # is an unused part of the key. The unused part of the key was in earlier versions used to supply the id of a publisher, this has since been moved to the message body, see “publisherId” in the next section.\nThe body of the trigger message has 3 relevant parameters:\ndataSourceId - Triggers the harvest of a specific source from fdk-harvest-admin publisherId - Triggers the harvest of all sources for the specified organization number. forceUpdate - Indicates that the harvest should be performed, even when no changes are detected in the source A triggered harvest will download all relevant sources from fdk-harvest-admin, download everything from the source and try to read it as a RDF graph via a jena Model. If the source is successfully parsed as a jena Model it will be compared to the last harvest of the same source. The harvest process will continue if the source is not isomorphic to the last harvest or forceUpdate is true.\nAll blank nodes will be skolemized in the resource graphs, which means that an URI is generated for the blank node.\nWhen all sources from the trigger has been processed a new rabbit message will be published with the routing key *.harvested, the message body will be a list of harvest reports, one report for each source from fdk-harvest-admin. Each report will contain these fields:\nid - the id for the source in fdk-harvest-admin url - the source url dataType - the relevant resource type harvestError - a boolean wich is set to true if the harvest failed startTime - the timestamp when the harvest started endTime - the timestamp when the harvest finished errorMessage - eventual error message if the harvest failed changedCatalogs - fdkId and uri for each catalog with changes changedResources - fdkId and uri for each resource with changes removedResources - fdkId and uri for each resource removed from the source Since subsequent part of the harvest process is dependent of kafka events, the services deployed from fdk-kafka-event-publisher will listen for harvest reports in rabbitMQ and produce kafka events for removed and changed resources.\n"},{"uri":"/data.norge.no/search/llm/","title":"LLM","tags":[],"description":"","content":"The service responsible for handling AI LLM searches is fdk-llm-search-service, which is described by this OpenAPI specification.\nEnvironments Production endpoint: https://aisearch.api.fellesdatakatalog.digdir.no/llm (not yet available) Demo endpoint: https://aisearch.api.demo.fellesdatakatalog.digdir.no/llm (not yet available) Staging endpoint: https://aisearch.api.staging.fellesdatakatalog.digdir.no/llm Querying Simple example using the staging endpoint:\ncurl -X POST \u0026#39;https://aisearch.api.staging.fellesdatakatalog.digdir.no/llm\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;Data about birds living in Norway\u0026#34;}\u0026#39; The results should look like this:\n{ \u0026#34;hits\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;d69e69bb-7572-3979-9298-d0bd1f4552fd\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;SEAPOP Estimerte hekkebestander for sjøfugl\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Dette datasettet inneholder estimerte hekkebestander for sjøfugl i Norge.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DATASET\u0026#34;, \u0026#34;publisher\u0026#34;: \u0026#34;Norsk institutt for naturforskning\u0026#34;, \u0026#34;publisherId\u0026#34;: \u0026#34;950037687\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;13f37cd2-8278-3812-b4cf-c2c9fcd7447a\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;SEAPOP Bestander for sjøfugl i åpent hav - vintersesong\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Dette datasettet inneholder bestander for sjøfugl i åpent hav i Norge i vintersesongen.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DATASET\u0026#34;, \u0026#34;publisher\u0026#34;: \u0026#34;Norsk institutt for naturforskning\u0026#34;, \u0026#34;publisherId\u0026#34;: \u0026#34;950037687\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;71c897bf-370a-3cd7-b5d1-f7485c773ec3\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;SEAPOP Bestander for sjøfugl i åpent hav - høstsesong\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Dette datasettet inneholder bestander for sjøfugl i åpent hav i Norge i høstsesongen.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DATASET\u0026#34;, \u0026#34;publisher\u0026#34;: \u0026#34;Norsk institutt for naturforskning\u0026#34;, \u0026#34;publisherId\u0026#34;: \u0026#34;950037687\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;9ca54865-8fcb-3f59-bbc5-c44bbbf10f04\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;SEAPOP Bestander for sjøfugl i åpent hav - sommersesong\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Dette datasettet inneholder bestander for sjøfugl i åpent hav i Norge i sommersesongen.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DATASET\u0026#34;, \u0026#34;publisher\u0026#34;: \u0026#34;Norsk institutt for naturforskning\u0026#34;, \u0026#34;publisherId\u0026#34;: \u0026#34;950037687\u0026#34; } ] } Limitations The service returns maximum 7 results. This is due to performance and size of the prompt. This number can change in the future while new models become faster.\nThe service expects to find datasets. In order to get the best results we recommend to query for specific type of data and not general questions like \u0026ldquo;What time does the sun rise?\u0026rdquo;.\n"},{"uri":"/data.norge.no/overordnet-arkitektur/m%C3%A5lbilde/","title":"Målbilde","tags":[],"description":"","content":"Vi jobber kontinuerlig med å oppdatere målbildet vårt etter hvert som behovene endrer seg. I hovedsak dreier endringene seg om å implementere støtte for nye typer kataloger.\nDiagram Sammenhengen mellom komponenten illustreres i følgende diagram: Om komponentene i målbildet Felles Datakatalog Portal: grafisk brukergrensesnitt der publikum har generell tilgang til oversikt over innhold som er publisert av hver enkelt virksomhet. Felles Datakatalog API: back-end komponenter som høster og lagrer fra ulike kilde-kataloger. Tilbyr API for å lese katalog-innhold. X-katalog: samling av kataloger som virksomheter har publisert. Disse komponentene tilbyr et maskin-til-maskin grensesnitt (API) mot de ulike katalogene. Understøtter høsting på standardiserte formater (feks DCAT-AP-NO, SKOS-AP-NO). X-katalog GUI: Et rollebasert brukergrensesnitt der virksomheten har tilgang til søk og oppslag i sin katalog. en x-katalog i virksomheten: lokale kataloger som er installert hos den enkelte virksomhet og vedlikeholdes og driftes av virksomheten. Understøtter høsting. Modellene Archi-modellen: https://github.com/Informasjonsforvaltning/SA_Informasjonsforvaltning Html-versjon: https://informasjonsforvaltning.github.io/SA_Informasjonsforvaltning/ "},{"uri":"/data.norge.no/overordnet-arkitektur/","title":"Overordnet arkitektur","tags":[],"description":"","content":"Her følger en kort oversikt over den overordnede arkitekturen til våre komponenter.\nTOGAF Vi følger en tilpasset versjon av TOGAFs ADM metodikk når vi jobber med arkitektur. Dette betyr at vi tar fram både baseline (Nåsituasjon) og target (Målbilde) modeller, og ut i fra disse utarbeider vi transisjoner og migrasjonsplaner. Transisjons-arkitekturer er å finne i målbildet.\nArchiMate Våre arkitekturmodeller er laget i henhold til Archimate-rammeverket med hjelp av verktøyet Archi. Kildekoden til modellene ligger åpent tilgjengelig for alle på våre Github-side. Hvordan vi har brukt Archi og en enkel oppskrift på hvordan ta dette i bruk vil du finne under de respektive arkitektur-modellene. Det finnes også en en html-versjon av modellene. Linken til disse vil du finne på sidene under.\nAPI-first Vi forsøker å utvikle våre komponenter etter “API”-first metodikken. Vi har derfor valgt å spesifisere våre API etter OpenAPI Specification v 3.\nÅpen kildekode Kildekoden til løsningen ligger åpent tilgjengelig på Github. All kildekode har en åpen lisens Apache License 2.0\n"},{"uri":"/developers_handbook/","title":"Developer&#39;s handbook","tags":[],"description":"","content":"The Developer\u0026rsquo;s handbook contains descriptions of our best practices developers will find useful when producing code in our projects. You will find few absolute rules, but you are strongly encouraged to follow practices described here.\n"},{"uri":"/data.norge.no/harvesting/harvest-admin/","title":"Harvest admin","tags":[],"description":"","content":"Backend fdk-harvest-admin\nFrontend fdk-admin-gui\nHarvest Admin is a service that provide functionality to register and list datasource endpoints to be harvested.\n"},{"uri":"/developers_handbook/tools_and_software/java_and_kotlin/","title":"Java and Kotlin","tags":[],"description":"","content":"Recommended Java Implementations: You can install the one of the versions listed below via SDKMAN or manually by following the installation guide on their website.\n\u0026ndash; Azul Zulu.\n\u0026ndash; Amazon Coretto\n\u0026ndash; Eclipse Temurin from Adoptium.\nSDKMAN You can easily install Java via SDKMAN. After installation you can use the command sdk list java to see available java versions. The version you use depends on your context, meaning which app you are trying to use it in. Therefore, always refer to the version mentioned in the README.md file of the repository. Other useful commands:\nList available Java implementations\nsdk list java Installing Java OpenJDK version\nsdk install java x.y.z-open Switching between different java versions\nsdk use java \u0026lt;version\u0026gt; Setting a default version\nsdk default java \u0026lt;version\u0026gt; Maven Maven can also be installed with SDKMAN or manually by following their installation guide: https://maven.apache.org/install.html\nInstalling maven with sdkman sdk install maven x.y.z Running a maven project: mvn clean install "},{"uri":"/data.norge.no/search/sparql/","title":"SPARQL","tags":[],"description":"","content":"Read more about SPARQL here.\nProduction endpoint: https://sparql.fellesdatakatalog.digdir.no Demo endpoint: https://sparql.demo.fellesdatakatalog.digdir.no Staging endpoint: https://sparql.staging.fellesdatakatalog.digdir.no\nSimple example using the staging endpoint:\ncurl -X POST \u0026#39;https://sparql.staging.fellesdatakatalog.digdir.no/?query=SELECT%20%2A%20WHERE%20%7B%20?sub%20?pred%20?obj%20.%20%7D%20LIMIT%201\u0026#39; Data.norge.no has a simple GUI for SPARQL queries:\nproduction https://data.norge.no/sparql demo https://demo.fellesdatakatalog.digdir.no/sparql staging https://staging.fellesdatakatalog.digdir.no/sparql Query examples List all properties and objects where the subject is this dataset https://staging.fellesdatakatalog.digdir.no/datasets/04edc67b-046c-37a8-9822-29f03d2f1e80:\nPREFIX dcat: \u0026lt;http://www.w3.org/ns/dcat#\u0026gt; PREFIX foaf: \u0026lt;http://xmlns.com/foaf/0.1/\u0026gt; PREFIX dct: \u0026lt;http://purl.org/dc/terms/\u0026gt; SELECT ?property ?object WHERE {​​​​​​ ?dataset a dcat:Dataset . ?record foaf:primaryTopic ?dataset . ?record a dcat:CatalogRecord . ?record dct:identifier \u0026#34;04edc67b-046c-37a8-9822-29f03d2f1e80\u0026#34; . ?dataset ?property ?object . }​​​​​​ List all dataset titles:\nPREFIX dcat: \u0026lt;http://www.w3.org/ns/dcat#\u0026gt; PREFIX dct: \u0026lt;http://purl.org/dc/terms/\u0026gt; SELECT ?title WHERE {​​​​​​​​​​​​​​ ?dataset a dcat:Dataset . ?dataset dct:title ?title . }​​​​​​​​​​​​​​​​​​​​​ "},{"uri":"/developers_handbook/tools_and_software/python/","title":"Python","tags":[],"description":"","content":"Setting up Python environment Resources mentioned in this guide:\npyenv (github) poetry (homepage) nox (homepage) nox-poetry (homepage) optional: pipx (homepage) Installation and setup (macOS) Install pyenv (docs) On Mac OS X you need Homebrew and the Xcode Command Line Tools:\nxcode-select --install Then run (Pyenv suggested build environment):\nbrew install openssl readline sqlite3 xz zlib tcl-tk Run the following commands:\nbrew update brew install pyenv Add the following lines to .zshrc:\nexport PYENV_ROOT=\u0026#34;$(pyenv root)\u0026#34; eval \u0026#34;$(pyenv init -)\u0026#34; Check that installation was succesful:\npyenv --version Should print the pyenv version, i.e. pyenv 2.3.36\nTo install the remaining packages and dependencies with the pipx CLI, continue reading Alternative A. For an alternative installation without pipx skip to Alternative B.\nAlternative A: Installation with pipx pipx pipx is a CLI to install and run Python-applications in isolated environments. It is intended for globally available installations of Python CLIs, like Poetry, and not for libraries and project dependencies.\nNote: pipx installed with brew is independent from the active python-version selected with pyenv.\nInstall pipx (installation-docs) To install, run the following commands:\nbrew update brew install pipx pipx ensurepath Check that pipx is available:\npipx --version Should print something like 1.4.3.\nCheck that the line\nexport PATH=\u0026#34;$PATH:/Users/\u0026lt;username\u0026gt;/.local/bin\u0026#34; has been added to your ~/.zshrc (or equivalent config file).\nNB! Make sure that you are not in an active virtual environment (i.e. by running poetry env use ...) when running pipx commands. Otherwise pipx might install packages to the wrong virtual environment.\nInstall Poetry with pipx (installation-docs) Run:\npipx install poetry Check that poetry is available:\npoetry --version This should print something like Poetry (version 1.7.1)\nMake sure Poetry uses the currently active python version from pyenv by running:\npoetry config virtualenvs.prefer-active-python true When in a Poetry project with a pyproject.toml this should list a pyenv version of python:\npoetry env info Nox Nox is a command line tool that automates testing in multiple Python environments.\nInstall nox with pipx pipx install nox nox-poetry nox-poetry is a package which makes you able to run Poetry inside Nox sessions.\nInstall nox-poetry with pipx To install it to the same environment as Nox is run from, run:\npipx inject nox nox-poetry Alternative B: Installation without pipx Make sure you have the expected python version activated with pyenv local x.x.x, and make sure that pip is pointing to that python version.\nThis will install the packages for each python version. In other words you have to install the following packages for each python-version you install and activate with pyenv.\nInstall Poetry pip install poetry Installing a specific version\npip install poetry==1.2.0 Nox pip install nox nox-poetry pip install nox-poetry Known problems VS Code can\u0026rsquo;t find virtual environments Make sure the project virtual environments created by Poetry are found by VS Code, by adding the following option to settings.json (macOS-specific):\n\u0026#34;python.venvPath\u0026#34;: \u0026#34;~/Library/Caches/pypoetry/virtualenvs\u0026#34; poetry config virtualenvs.path gives the venv-path on your on OS.\nMissing system library: lxml Error message when running a nox session:\n.cache/contract_tests-3-8/lib/python3.8/site-packages/feedgen/feed.py:17: in \u0026lt;module\u0026gt; from lxml import etree # nosec - not using this for parsing E ImportError: dlopen(/Users/xxxx/Digdir/fdk-fulltext-search/.cache/contract_tests-3-8/lib/python3.8/site-packages/lxml/etree.cpython-38-darwin.so, 0x0002): symbol not found in flat namespace \u0026#39;_exsltDateXpathCtxtRegister\u0026#39; Install the following packages:\nbrew install libxml2 brew install libxslt We need to install an older version of lxml. Add the following line to your pyproject.toml.\nlxml = \u0026#34;4.9.2\u0026#34; Docker command not found When using podman and docker-compose it is possible you get the following error running tests:\nCommand: docker not found Link podman to docker:\nsudo ln -s /opt/podman/bin/podman /usr/local/bin/docker Podman/docker crashes Remember to have setup enough resoures for your Podman machine.\nUseful commands pyenv Install a specific Python version\npyenv install 3.11 Set the local python version for the current folder/project\npyenv local 3.11 This creates a .python-version file in the current directory.\nChange python version globally (for user)\npyenv global 3.11 This is used as the default interpreter when no local python version is activated.\n"},{"uri":"/developers_handbook/testing/","title":"Testing","tags":[],"description":"","content":"On testing microservices and how we do it.\nFrom Clemson \u0026ldquo;Testing Strategies in a Microservice Architecture\u0026rdquo; we have borrowed the following strategies:\nunit integration component contract end-to-end From Clemson \u0026ldquo;Testing Strategies in a Microservice Architecture\u0026rdquo; we have borrowed the following strategies:\nunit (solitary + sociable) automated, included in coverage integration (testing adapters, classes that communicates with the outside world) automated, included in coverage contract (based on specification, \u0026ldquo;service test\u0026rdquo;, the artifact that will be deployed) automated end-to-end (minimal) mostly manual next step\nAs an alternative to end-to-end tests we prefer a technique called semantic monitoring, where we use fake events (synthetic transactions) to ensure that the system is behaving semantically.\nAnother aspect of testing is timing, when to run what tests: References https://martinfowler.com/articles/microservice-testing/ https://samnewman.io/books/building_microservices/ https://martinfowler.com/bliki/SyntheticMonitoring.html "},{"uri":"/developers_handbook/git_workflow/","title":"Git workflow","tags":[],"description":"","content":"There are different popular Git workflows. We consider Gitflow too focused on scheduled releases. Since we try to deploy \u0026ldquo;all the time\u0026rdquo;, we shall adhere to the GitHub Flow:\nAnything in the master branch is deployable To work on something new, create a descriptively named branch off of master (ie: new-oauth2-scopes) Commit to that branch locally and regularly push your work to the same named branch on the server When you need feedback or help, or you think the branch is ready for merging, open a pull request After someone else has reviewed and signed off on the feature, you can merge it into master Once it is merged and pushed to \u0026lsquo;master\u0026rsquo;, you can and should deploy immediately Tips and tricks Nice introductction to GitHub flow here: https://docs.github.com/en/get-started/quickstart/github-flow "},{"uri":"/developers_handbook/tools_and_software/rust/","title":"Rust","tags":[],"description":"","content":"\nTo install Rust, refer to the Installing Rust section within the Getting Started page. The Rust Book is a great place to start learning Rust. "},{"uri":"/developers_handbook/code_quality/","title":"Code quality","tags":[],"description":"","content":"We use SonarCloud to monitor code quality, including test coverage.\nFirst and foremost code quality analysis is a tool for code review. In the following pages you will find a brief discussion on what we consider important aspects of quality when doing a review.\n"},{"uri":"/developers_handbook/tools_and_software/go/","title":"Go","tags":[],"description":"","content":"\nTo install Go, refer to the Download and install section within the Go User Manual. The Getting started section is a great place to start learning Go. "},{"uri":"/data.norge.no/metadata-quality/","title":"Metadata quality","tags":[],"description":"","content":" d a t a s e t K - a e f v k d e a a n t t a - s p e u t b - l e i v s e h n a e t s r s s m e n t a m t R q o a a r b - b h d i a a t r t M v a Q e s s e t t P s - o e s v t e g n r t e s s d a p t r d a o u c s p r a e e l t s t r - - c - t c v o h y h a r a - e l i r c c i n v h k d g e e e a - s c r t a t k o p e e r i r r m q a - s e c v o e r n i t n s g - s e r v i c e dataset-event-publisher listens for RabbitMQ harvest messages from dataset-harvester. It then gets dataset-graphs from dataset-harvester, and produces an event on the dataset-events topic for every dataset dataset-harvester has updated. assmentator consumes every harvested graph from dataset-events, adds hasAssessment properties on both dataset- and distribution-nodes, and produces a new event with the resulting graph on mqa-dataset-events. property-checker, url-checker and dcat-validator will all three consume dataset graphs from mqa-dataset-events and produce assessment graphs to mqa-events. These assessment graphs is consumed by scoring-service, and merged based on fdk_id and event timestamp. scoring-service saves/updates assessment graphs in Postgres through scoring-api, as well as total score for every dimension so that it is possible to execute quick aggregated queries.\n"},{"uri":"/developers_handbook/operations_and_status_codes/","title":"Operations and status codes","tags":[],"description":"","content":"The following is guidelines prescribing what HTTP method to use for common \u0026ldquo;business functions\u0026rdquo; and what status codes to return for the happy case. If a header is to be returned, an example is given.\nFunction HTTP Method Status Code Header Comment List GET /foos/ 200 OK Collection in response. 200 Ok even if empty collection Create POST /foos/ 201 Created Location: /foos/1234 No body in response Get GET /foos/1234 200 OK Update PUT /foos/1234 204 No Content No body in response Update (patch) PATCH /foos/1234 200 OK The updated resource in response body Delete DELETE /foos/1234 204 No Content No body in response References Method definitions Status Code Definitions\nJSON Patch\n"},{"uri":"/data.norge.no/search/","title":"Search","tags":[],"description":"","content":"There are 3 different ways to search for data in Data.norge.no, each is based on a different technology.\nSearch The main search service in Data.norge.no is based on ElasticSearch. This service searches the text in a limited selection of fields, and has filters and other advanced functionality that helps users navigate.\nSee this page for more info\nLLM The LLM search service uses a \u0026lsquo;Large Language Model\u0026rsquo; (LLM) that is built from the same data used in the main search, but will accept naturally worded queries where the other search demands exact wording of titles or keywords.\nSee this page for more info\nSPARQL The SPARQL search service is based on the RDF query language SPARQL, and is our most advanced and powerful search. A user will have to create queries that follows the correct syntax, but will be able to search all harvested data points, not just pre-selected fields.\nSee this page for more info\n"},{"uri":"/developers_handbook/tools_and_software/","title":"Tools and software","tags":[],"description":"","content":"The following is what is intended as useful hints and howtos when you set up your computer for working with our projects. The following tips are in no way requirements, but should help you to get started painlessly.\n"},{"uri":"/data.norge.no/harvesting/","title":"Harvesting","tags":[],"description":"","content":"\nThe harvest solution consists of 3 parts:\nHarvest - The resources are downloaded from several sources, split into single resource-graphs and given an id Reason - Each resource graph is enriched with relevant data Parse - Each resource graph is converted to JSON The finished JSON-representation of each resource is then picked up by both the backend for the search page, fdk-search-service, and the backend for the details page, fdk-resource-service. Changes in the resources are available in data.norge when these two services have been updated.\nThe harvest process can be initiated by two services:\nHarvest scheduler - Initiates harvest of all sources from predefined schedules. Harvest admin - This is where sources are registered for harvest, each source has a button to initiate harvest of that specific source. The communication between the relevant services is handled by a combination of RabbitMQ and Apache Kafka. The harvests are triggered by messages published in RabbitMQ, and the harvesters will publish harvest reports for each source in RabbitMQ when they are done. These reports contain information about each resource with changes and each resource that has been removed from the source since the last harvest. These reports are picked up by different versions of fdk-kafka-event-publisher, and will produce events in Kafka for each changed resource. Reasoning consumes events about changed resources and produces new events with the enriched graphs. Parsing consumes events about reasoned resources and produces new events with a JSON version of the resource. The events about parsed resources are consumed by fdk-search-service and fdk-resource-service and the harvest process is finished.\nParts of FDK not strictly part of the harvest process that are also dependent of the kafka events produced by the process:\nfdk-sparql-service Listens for reasoned and removed messages to maintain updated graphs available for sparql queries Metadata quality Listens for DATASET_HARVESTED to produce an assessment of the harvested datasets Detailed schema of the harvest process "},{"uri":"/developers_handbook/tools_and_software/kafka/","title":"Kafka","tags":[],"description":"","content":"Read messages from Kafka Number of partitions and their message count Substitute dataset-events with the desired topic.\nkubectl exec -it kafka-1-0 -- /bin/kafka-run-class kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic dataset-events You may omit --topic to list all topics, but should then ignore __consumer and _schemas.\nkubectl exec -it kafka-1-0 -- /bin/kafka-run-class kafka.tools.GetOffsetShell --broker-list localhost:9092 | grep -v __consumer | grep -v _schemas Example output dataset-events:0:47472 dataset-events:1:49949 dataset-events:2:49361 dataset-events:3:50122 Read last n messages from partition in topic Substitute topic, partition and offset parameters based on output from command above. Remember that all messages with the same key (often some sort of ID) is put on the same partition.\nkubectl exec -it schema-registry-1-0 -- kafka-avro-console-consumer --bootstrap-server kafka-1:9092 --property schema.registry.url=http://localhost:8081 --topic dataset-events --partition 0 --offset 47470 You may redirect the command output to a file and then use jq to get a more readable output.\n# Pipe output into file \u0026#39;kafka-output\u0026#39; while also seeing output in terminal kubectl exec ... | tee kafka-output # Show whole messages cat kafka-output | grep \u0026#34;{\u0026#34; | jq # Show selected fields in messages cat kafka-output | grep \u0026#34;{\u0026#34; | jq \u0026#34;{type,fdkId,timestamp}\u0026#34; "},{"uri":"/_header/","title":"","tags":[],"description":"","content":"\n"},{"uri":"/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"/","title":"Data.norge.no documentation","tags":[],"description":"","content":"Data.norge.no documentation Here you can find more information and documentation about Data.norge.no.\nIf you find something that is wrong or not described good enough in our documentation:\nsend an email to fellesdatakatalog@digdir.no create an issue in Github Informasjonsforvaltning/docs create a pull request in Github Informasjonsforvaltning/docs If you find something that is wrong or missing in Data.norge.no:\nsend an email to fellesdatakatalog@digdir.no create an issue in Github Informasjonsforvaltning/fdk-issue-tracker. "},{"uri":"/tags/","title":"Tags","tags":[],"description":"","content":""}]