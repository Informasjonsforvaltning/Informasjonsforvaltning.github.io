[{"uri":"/data.norge.no/bakgrunn-f%C3%B8ringer-effekter/","title":"Bakgrunn, føringer og effekter","tags":[],"description":"","content":"Felles offentlig informasjonsforvaltning Beslutninger i offentlig sektor tas på grunnlag av informasjon innhentet fra brukere, tredjeparter og fra andre offentlige virksomheter. Med god dokumentering av hvilken informasjon den enkelte offentlige virksomhet forvalter og åpne APIer som gir tilgang til selve dataene, kan andelen som brukerne selv rapporterer inn gradvis reduseres.\nTiltak over flere trinn Tiltakene innen informasjonsforvaltning plasserers ofte i følgende \u0026ldquo;ambisjonstrapp\u0026rdquo; hentet fra Difis sider om informasjonsforvaltning:\nTilrettelegge - \u0026ldquo;Orden i eget hus\u0026rdquo; Skaffe oversikt over den informasjon som virksomhetene forvalter = beskrive denne på en enhetlig måte gjennom begrepsarbeid og gode definisjoner, samt vise sammenhenger gjennom informasjonsmodeller. Synliggjøre beskrivelsene for andre i form av datasett som blir publisert i en felles datakatalog. Herunder beskrivelser av kvalitet, opprinnelse/kontekst, lovgrunnlag, behandlinger og vedtak.\nTilgjengeliggjøre oversikten Beskrive hvordan data som er beskrevet i datasettene kan nås og anvendes – i en maskinlesbar og automatisert form – via en felles API-katalog. Også for data der tilgangen er begrenset (ikke åpne data), men som likevel kan nås dersom nærmere vilkår er oppfylt (hjemmel, registreringer, betalinger osv)\nAnvendelse - kun én gang (\u0026ldquo;Once only\u0026rdquo;) Etter hvert som oversiktene blir mer komplette og katalogene fylles opp av virksomhetene, vil det være grunnlag for mer automatisert gjenbruk og viderebruk av data, og mulig å nå målet om at informasjon bare skal innhentes én gang (deretter gjenbrukes).\nFøringer Digitaliseringsrundskrivet er en sammenstilling av pålegg og anbefalinger om digitalisering i offentlig sektor. Rundskrivet sier blant annet følgende: \u0026ldquo;Virksomheten skal registere datasett i Felles datakatalog (data.norge.no). Dette skal som et minimum gjøres når virksomheten endrer eller etablerer tjenester, herunder etablerer nye, eller oppgraderer eksisterende fagsystemer eller digitale tjenester.\u0026rdquo;\nEffekter Effektene av arbeidet med informasjonsforvaltning gjennom disse tiltakene vil blant annet være økt datakvalitet - felles definisjoner/forståelse og mere sanntidsdata. Dette vil igjen åpne for at rapporteringsplikter (skjema) kan reduseres i antall og omfang.\nVidere vil mer av rapporteringen kunne foregå med utgangspunkt i maskinelle og automatiserte løsninger (uttrekk og høsting av oppdaterte og riktige data basert på modeller). Herunder en bedre integrasjon mot andre eksisterende løsninger og muligheter for mer samhandling slik at rapporteringene rettes mot brukernes egne prosesser (brukerorientering).\nMed bedre beskrivelser og maskinlesbarhet vil det også ligge mer til rette for analyser av data og anvendelser innen kunstig intelligens (AI), noe som ytterligere styrker mulighetene for innovasjon. Både i form av mer bruk av sanntids informasjon og i form av sammensatte brukerorienterte tjenester.\nPå sikt kan det også føre til at noen rapporteringsplikter forsvinner fordi data fanges i det de oppstår og følger gjennom hele prosesser (sensorer – \u0026ldquo;tingenes internett\u0026rdquo;, transaksjoner – bestilling, kjøp/salg, fakturering, regnskap…)\nArbeidet med informasjonsforvaltning resulterer altså i et helt annet mulighetsrom når det gjelder anvendelse av data. Enten dette gjelder gjenbruk i rapporteringssammenheng eller innovasjon og nye forretningsområder. Det er gjort mange forsøk på å beskrive verdien av en slik effektiv bruk av data, f.eks. i USA (McKinsey) og EU, og dette beløper seg til mange milliarder. Uansett, det forutsetter at informasjonen er beskrevet og tilgjengeliggjort – i vårt tilfelle blant annet at de felles katalogene blir fylt med innhold.\n"},{"uri":"/developers_handbook/code_quality/coverage/","title":"Coverage","tags":[],"description":"","content":"Test coverage numbers are not the goal in itself. What is important to monitor is whether or not a pull request increases or decreases coverage. In most all cases a pull request should maintain or increase coverage. In extreme cases (e.g. when fixing a critical bug), it might be necessary to temporarily sacrifice coverage.\n"},{"uri":"/data.norge.no/","title":"Data.norge.no","tags":[],"description":"","content":"Data.norge.no is a solution that gives an overview of datasets, concepts, APIs, information models, services and events (and relations between these) in the public sector.\nHow can I test Data.norge.no? This is our demo environment: https://demo.fellesdatakatalog.digdir.no\nThis is our solution for registering resources in the demo environment: https://registrering.demo.fellesdatakatalog.digdir.no\nTo get a demo-user you can contect us at this email: fellesdatakatalog@digdir.no\nWhere can I read more? About the national data catalog About the search API "},{"uri":"/developers_handbook/code_organization/frontend/","title":"Front-end","tags":[],"description":"","content":"For our front-end (GUI) services implemented in React , a typical folder structure should be\u0026hellip;\n"},{"uri":"/developers_handbook/prepare_your_computer/linux/","title":"Linux","tags":[],"description":"A howto decsribing how to create and run a linux virtual machine","content":"HOWTO create and run a linux developer virtual machine on your non-linux laptop Install VMware Workstation Player for Windows 64-bit Operating Systems Download Ubuntu Desktop iso Open VMware Workstation Player and create a new Virtual Machine In the dialogue, choose \u0026ldquo;I will install the operating system later\u0026rdquo;: Choose \u0026ldquo;Linux\u0026rdquo; Version Ubuntu 64-bit Give it a suitable name and Location should be on your computer, preferably a SSD. Do NOT install on network-drive. Maximum disk size should be set to e.g. 100GB, single file is OK. Review your settings and press Finish. Select your new virtual machine, and press \u0026ldquo;Edit virtual machine settings\u0026rdquo;. Under Hardware, choose CD/DVD (SATA), choose \u0026ldquo;Use ISO image file:\u0026rdquo; and point to the location on your disc with the iso image from step 2. Memory: \u0026gt; 8GB Processors: 4 cores. Check the box for \u0026ldquo;Virtualize Intel VT-x/EPT or AMD-V/RVI\u0026rdquo; Press OK. Select your new virtual machine, and press \u0026ldquo;Play virtual machine\u0026rdquo;. Installation of os will start. Follow instructions. They should be self explained. Choose \u0026ldquo;Norsk bokmål\u0026rdquo; and press \u0026ldquo;Installer Ubuntu\u0026rdquo; After successful installation of os, your virtual machine will reboot. Make sure the ISO image file is removed from the settings. "},{"uri":"/developers_handbook/microservices/","title":"Microservices","tags":[],"description":"","content":"We base our understanding of what a microservice is on the following definitions:\nMicroservices are small, autonomous services that work together.\nServices are out-of-process components who communicate with a mechanism such as a web service request, or remote procedure call.\nA component is a unit of software that is independently replaceable and upgradeable.\nReferences Sam Newman, Building Microservices\nMartin Fowler, Microservices\n"},{"uri":"/data.norge.no/overordnet-arkitektur/n%C3%A5situasjon/","title":"Nåsituasjon","tags":[],"description":"","content":"Her følger en kort oversikt over den overordnede arkitekturen til våre komponenter.\nDiagram Sammenhengen mellom komponentene illustreres i følgende diagram: Om komponentene i nåsituasjon I dag har vi delt de ulike komponentene inn i en løsning for søk i kataloger, en løsning for registrering, en løsning for innhøsting (harvester). I tillegg har vi en intern komponent for referanse-data.\nSøkeløsningen search: Vår portal for søk på tvers av alle katalogene search-api: Eksternt api med oppslag mot datasettkatalogen. api-cat: Eksternt api med oppslag mot api-katakogen. concept-cat: Eksternt api med oppslag mot begreps-katalogen. Registreringsløsningen registration-react: Vårt brukergrensesnitt for å registrere og legge inn innhold i kataloger registration-api: backend-komponent for Registreringsløsningen registration-auth: komponent som håndterer innlogging via IDPorten Høsteløsningen Høsteløsningen laster ned datakataloger og gjør dem søkbare. Består av\nharvester harvester-api Modellene Archi-modellen: https://github.com/Informasjonsforvaltning/SA_Informasjonsforvaltning_BASELINE Html-versjon: https://informasjonsforvaltning.github.io/SA_Informasjonsforvaltning_BASELINE/ "},{"uri":"/developers_handbook/monitoring/probes/","title":"Probes","tags":[],"description":"","content":"Probes should not require authorization.\nAny http response code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure.\nReadiness A containers\u0026rsquo;s readiness probe is used to check whether the service is ready to accept connections.\nExample:\nreadinessProbe: httpGet: path: /readyz port: 8080 initialDelaySeconds: 3 periodSeconds: 30 successThreshold: 1 failureThreshold: 3 We should check if the runtime dependencies that the service absolutely need to function, is ready. If the service depends on a database, it should check if the database is ready.\nLiveness A containers\u0026rsquo;s liveness probe is used to check whether the service has gone into a broken state and should be restarted.\nExample:\nlivenessProbe: httpGet: path: /livez port: 8080 initialDelaySeconds: 5 periodSeconds: 30 successThreshold: 1 failureThreshold: 5 An implementation (in java/kotlin) would be simply\n@RequestMapping(value = [\u0026#34;/ping\u0026#34;], method = [GET]) fun ping(): ResponseEntity\u0026lt;Void\u0026gt; = ResponseEntity.ok().build() References The following is a nice intro for Spring Boot: https://www.baeldung.com/spring-boot-kubernetes-self-healing-apps\nKubernetes configuration: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/\nExample with authorization: https://www.critiqus.com/post/kubernetes-health/\n"},{"uri":"/data.norge.no/harvesting/rdf-parse/","title":"RDF parsing","tags":[],"description":"","content":"RDF parsing consists of three components:\nRDF Parser Parser Library written in Python Repository: fdk-rdf-parser\nRDF Parser Service JSON API service responsible for parsing RDF (turtle) to JSON using RDF Parser. Repository: fdk-rdf-parser-service\nRDF Parse Event Publisher Kafka consumer/producer consuming events of type reasoned and producing RDF Parse Events using RDF Parser Service. Repository: fdk-rdf-parse-event-publisher\n"},{"uri":"/data.norge.no/search/search/","title":"Search","tags":[],"description":"","content":"The service responsible for handling searches is fdk-search-service, which is described by this OpenAPI specification.\nProduction endpoint: https://search.api.fellesdatakatalog.digdir.no/search Demo endpoint: https://search.api.demo.fellesdatakatalog.digdir.no/search Staging endpoint: https://search.api.staging.fellesdatakatalog.digdir.no/search\nSimple example using the staging endpoint:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;}\u0026#39; Searchable fields There are 3 searchable fields, they are title, description and keyword. The service will by default try to find matches for the query in all 3 fields, but it\u0026rsquo;s possible to define which of the fields it should include in the search body.\nExample where only hits from the description fields are included:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;, \u0026#34;fields\u0026#34;: {\u0026#34;title\u0026#34;:false,\u0026#34;description\u0026#34;:true,\u0026#34;keyword\u0026#34;:false}}\u0026#39; Boosting Hits from some fields will be prioritized over others, i.e. a matching hit from the title field will be prioritized over a hit from the description field.\nField Boost title, full phrase match 30 title, partial match 15 keyword 5 description 1 Take the title \u0026ldquo;Test search service\u0026rdquo; and the two queries \u0026ldquo;test service\u0026rdquo; and \u0026ldquo;search service\u0026rdquo;. The first query will have 2 partial matches \u0026ldquo;test\u0026rdquo; and \u0026ldquo;service\u0026rdquo;, with a combined search value of 15 + 15 = 30, the second query will have 3 matches where two are partial, \u0026ldquo;search\u0026rdquo; and \u0026ldquo;service\u0026rdquo;, and one is a full phrase match, \u0026ldquo;search service\u0026rdquo;, with a combined search value of 15 + 15 + 30 = 60.\nSpecific resource types Each resource type has it\u0026rsquo;s own endpoint, the available endpoints are /datasets, /data-services, /concepts, /information-models, /events and /services\nExample using the datasets endpoint:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search/datasets\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;}\u0026#39; Pagination All search results will be paginated, it is possible to customize the size and page number with the pagination field in the search body.\nExample using the pagination field, with current page set to number 5 and there are 10 hits per page:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;pagination\u0026#34;:{\u0026#34;size\u0026#34;:10,\u0026#34;page\u0026#34;:5}}\u0026#39; Filtering It\u0026rsquo;s possible to filter the search result, see SearchFilters in the OpenAPI specification for a list of all possible filters and what type of value they accept.\nExample using the data theme filter:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;filters\u0026#34;:{\u0026#34;dataTheme\u0026#34;:{\u0026#34;value\u0026#34;:[\u0026#34;ENVI\u0026#34;]}}}\u0026#39; Example using the open data filter:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;filters\u0026#34;:{\u0026#34;openData\u0026#34;:{\u0026#34;value\u0026#34;:true}}}\u0026#39; Example using the formats filter:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;filters\u0026#34;:{\u0026#34;formats\u0026#34;:{\u0026#34;value\u0026#34;:[\u0026#34;MEDIA_TYPE application/json\u0026#34;]}}}\u0026#39; Aggregations Each search result will include aggregations of the query for possible filter values. There are always included a value for each filter, it\u0026rsquo;s a list of the filter options represented in the total search result and a count of how many hits the filter option has.\nGiven that this search:\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;}\u0026#39; Has this as the aggregation in the result:\n\u0026#34;aggregations\u0026#34;:{\u0026#34;accessRights\u0026#34;:[{\u0026#34;key\u0026#34;:\u0026#34;PUBLIC\u0026#34;,\u0026#34;count\u0026#34;:5},{\u0026#34;key\u0026#34;:\u0026#34;RESTRICTED\u0026#34;,\u0026#34;count\u0026#34;:16}]} Then the next example would therefore have 5 hits in it\u0026rsquo;s result, since the aggregation values shows that the query has 5 hits where the value for the access rights field is PUBLIC and 16 where the value is RESTRICTED.\ncurl -X POST \u0026#39;https://search.api.staging.fellesdatakatalog.digdir.no/search\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;filters\u0026#34;:{\u0026#34;accessRights\u0026#34;:{\u0026#34;value\u0026#34;:\u0026#34;PUBLIC\u0026#34;}}}\u0026#39; "},{"uri":"/behovsprosessen/","title":"Åpen behovsprosess","tags":[],"description":"","content":"Vår behovsprosess er implementert som et GitHub-prosjekt og er åpent tilgjengelig. Behovsprosessen starter ved at det er registrert et issue på en av våre løsninger eller på vårt dedikerte behovsprosjekt.\nDersom du ønsker å registrere et behov eller en brukerhistorie mot en av våre løsninger, kan du opprette et nytt issue ved å opprette en brukerhistorie.\nBehovsprosessen har følgende steg:\nAvklare behov Kriteria: Issuet skal vere identifisert som et potensielt behov.\nAktiviterer:\nAnsvarlig rolle Aktivitet Verktøy Gevinstansvarlig Opprette epos og utarbeide enkel gevinstanalyse GitHub Funksjonell arkitekt Opprette brukerhistorier GitHub Analysere behov Kriteria: Behovet skal vere formulert som et epos og ha en enkel gevinstanalyse. Brukerhistorien skal tildeles forretningsverdi etter følgende kriteria:\nIndikator Kriterium Kritisk Eposet kan ikke leveres uten denne brukerhistorien. Viktig Brukerhistorien er vesentlig for at funksjonaliteten i Eposet realiseres. Mindre viktig Eposets funksjonelle intensjon er realisert uten denne brukerhistorien. Kjekt-å-ha Når alt annet er levert og teamet ikke har noe bedre å finne på, kan denne realiseres. Aktiviteter:\nAnsvarlig rolle Aktivitet Verktøy Funksjonell arkitekt Detaljere epos og brukerhistorier som operasjonaliserer eposet GitHub Løsningsarkitekt Dokumentere relevante kvalitetskrav Interaksjonsdesigner Lage trådskisser der dette er relevant Beskrive design og løsningsarkitektur Kriteria: Eposet skal ha ei eller flere brukerhistorier knytta til seg.\nAktiviteter:\nAnsvarlig rolle Aktivitet Verktøy Løsningsarkitekt Utarbeide løsningsarkitektur Archi Designer Utarbeide design basert på trådskisser Testleder Utarbeide akseptansekriteria som features i Gherkin GitHub Senior utvikler Utarbeide scenarier i features GitHub Klar til konstruksjon Kritieria: Brukerhistorien skal ha et design (der det er relevant) og en løsningsarkitektur. Akseptansekriteria skal være dokumentert som et sett av features.\nAktiviteter:\nAnsvarlig rolle Aktivitet Verktøy Produkteier Prioritere brukerhistorier GitHub Scrumleder Opprette brukerhistorien i utviklingsprosjekt GitHub Seniorutvikler Dele brukerhistorien opp i oppgaver GitHub I konstruksjon Kritieria: Brukerhistorien er prioritert og opprettet i utviklingsprosjekt.\nHer vil brukerhistorien bli duplisert over i det relevante utviklingsprosjektet, som oppretter sine brukerhistorier etter behov. Disse skal referere til den opprinnelige brukerhistorien i behovsprosjektet. Utviklingsteamet vil også legge brukerhistorien ut i sine prosjekter med tilhørende milepæler.\nKlar til akseptansetest Kriteria: Utviklingsteamet har levert funksjonalitet til staging-miljø som realiserer brukerhistorien.\nAktiviteter:\nAnsvarlig rolle Aktivitet Verktøy Funksjonell arkitekt Godkjenne levert funksjonalitet Løsningsarkitekt Godkjenne levert kvalitet Produkteier Akseptere levert funksjonalitet Utført Kriteria: Funksjonalitet og kvalitet er godkjent i akseptansetest og issue er \u0026ldquo;closed\u0026rdquo;. Eller det er bestemt at behovet ikke skal taes videre i behovsprosessen. Alle issues som blir lukka vil automatisk havne i denne kolonnen.\n"},{"uri":"/developers_handbook/code_organization/backend/","title":"Back-end","tags":[],"description":"","content":"For our \u0026ldquo;back-end\u0026rdquo; services implemented in java/kotlin, a typical folder structure should be:\ntree -L 6 --dirsfirst -I \u0026#39;target\u0026#39; . ├── src │ ├── main │ │ ├── java │ │ │ └── no │ │ │ ├── brreg │ │ │ │ └── Generated.java │ │ │ └── template │ │ │ ├── model │ │ │ ├── spring │ │ │ └── Application.java │ │ ├── kotlin │ │ │ └── no │ │ │ └── template │ │ │ ├── controller │ │ │ ├── jena │ │ │ ├── mapping │ │ │ ├── repository │ │ │ └── service │ │ └── resources │ │ ├── openAPI │ │ │ └── openapi-generator-maven-plugin │ │ │ └── templates │ │ ├── specification │ │ │ └── template.yaml │ │ ├── application.properties │ │ ├── banner.txt │ │ └── gitBranchName.sh │ └── test │ ├── java │ │ └── no │ │ └── template │ │ ├── controller │ │ ├── integration │ │ ├── service │ │ └── TestResponseReader.java │ ├── kotlin │ │ └── no │ │ └── template │ │ └── TestData.kt │ └── resources │ ├── mockito-extensions │ │ └── org.mockito.plugins.MockMaker │ └── responses │ ├── example.ttl ├── buildCommands.sh ├── docker-compose.yml ├── Dockerfile ├── Jenkinsfile ├── pom.xml ├── README.md ├── TODO.md A reference implementation is here: https://github.com/Informasjonsforvaltning/a-back-end-service\n"},{"uri":"/developers_handbook/code_organization/","title":"Code organization","tags":[],"description":"","content":"We organize our code in a multi-repo way. That means that one microservice gets one git repository.\n"},{"uri":"/data.norge.no/harvesting/harvesters/","title":"Harvesters","tags":[],"description":"","content":" fdk-concept-harvester fdk-dataservice-harvester fdk-dataset-harvester fdk-event-harvester fdk-informationmodel-harvester fdk-public-service-harvester The harvest process is triggered by messages from RabbitMQ with the routing key *.#.HarvestTrigger, where * is the relevant resource type, ie dataset, and # is an unused part of the key. The unused part of the key was in earlier versions used to supply the id of a publisher, this has since been moved to the message body, see “publisherId” in the next section.\nThe body of the trigger message has 3 relevant parameters:\ndataSourceId - Triggers the harvest of a specific source from fdk-harvest-admin publisherId - Triggers the harvest of all sources for the specified organization number. forceUpdate - Indicates that the harvest should be performed, even when no changes are detected in the source A triggered harvest will download all relevant sources from fdk-harvest-admin, download everything from the source and try to read it as a RDF graph via a jena Model. If the source is successfully parsed as a jena Model it will be compared to the last harvest of the same source. The harvest process will continue if the source is not isomorphic to the last harvest or forceUpdate is true.\nAll blank nodes will be skolemized in the resource graphs, which means that an URI is generated for the blank node.\nWhen all sources from the trigger has been processed a new rabbit message will be published with the routing key *.harvested, the message body will be a list of harvest reports, one report for each source from fdk-harvest-admin. Each report will contain these fields:\nid - the id for the source in fdk-harvest-admin url - the source url dataType - the relevant resource type harvestError - a boolean wich is set to true if the harvest failed startTime - the timestamp when the harvest started endTime - the timestamp when the harvest finished errorMessage - eventual error message if the harvest failed changedCatalogs - fdkId and uri for each catalog with changes changedResources - fdkId and uri for each resource with changes removedResources - fdkId and uri for each resource removed from the source Since subsequent part of the harvest process is dependent of kafka events, the services deployed from fdk-kafka-event-publisher will listen for harvest reports in rabbitMQ and produce kafka events for removed and changed resources.\n"},{"uri":"/data.norge.no/search/llm/","title":"LLM","tags":[],"description":"","content":"TODO\n"},{"uri":"/data.norge.no/overordnet-arkitektur/m%C3%A5lbilde/","title":"Målbilde","tags":[],"description":"","content":"Vi jobber kontinuerlig med å oppdatere målbildet vårt etter hvert som behovene endrer seg. I hovedsak dreier endringene seg om å implementere støtte for nye typer kataloger.\nDiagram Sammenhengen mellom komponenten illustreres i følgende diagram: Om komponentene i målbildet Felles Datakatalog Portal: grafisk brukergrensesnitt der publikum har generell tilgang til oversikt over innhold som er publisert av hver enkelt virksomhet. Felles Datakatalog API: back-end komponenter som høster og lagrer fra ulike kilde-kataloger. Tilbyr API for å lese katalog-innhold. X-katalog: samling av kataloger som virksomheter har publisert. Disse komponentene tilbyr et maskin-til-maskin grensesnitt (API) mot de ulike katalogene. Understøtter høsting på standardiserte formater (feks DCAT-AP-NO, SKOS-AP-NO). X-katalog GUI: Et rollebasert brukergrensesnitt der virksomheten har tilgang til søk og oppslag i sin katalog. en x-katalog i virksomheten: lokale kataloger som er installert hos den enkelte virksomhet og vedlikeholdes og driftes av virksomheten. Understøtter høsting. Modellene Archi-modellen: https://github.com/Informasjonsforvaltning/SA_Informasjonsforvaltning Html-versjon: https://informasjonsforvaltning.github.io/SA_Informasjonsforvaltning/ "},{"uri":"/developers_handbook/monitoring/metrics/","title":"Metrics","tags":[],"description":"","content":"101 Prometheus is used to scrape /metric endpoints of applications running in the cluster. Metrics are numeric measurements, which when scraped over time become time series that describe how the given application is running.\nMetrics follow the following notation:\n\u0026lt;metric name\u0026gt;{\u0026lt;label name\u0026gt;=\u0026lt;label value\u0026gt;, ...} A gauge is a metric that represents a single numerical value that can arbitrarily go up and down. The following example gauge metric indicates that http://data.norge.no/ returns the http status code 200:\nprobe_http_status_code{ingress=\u0026#34;ingress-prod-v4\u0026#34;, instance=\u0026#34;http://data.norge.no/\u0026#34;, namespace=\u0026#34;prod\u0026#34;} 200 Another example is a counter metric, whose value may only increase or be reset to zero:\nprocessed_mail_requests{fdk_service=\u0026#34;fdk-mail-sender-service\u0026#34;, status=\u0026#34;success\u0026#34;} 7 One may based on metrics create rules that trigger alerts whenever an expression is true. E.g. the expression probe_http_status_code{} \u0026gt;= 500, which indicates than an application is unable to respond correctly.\nSee Prometheus Introduction and Metric Types for a more thorough introduction.\nMetric services Service Purpose https://prometheus.fellesdatakatalog.digdir.no/rules Alert rules overview https://karma.fellesdatakatalog.digdir.no Alerting dashboard https://thanos.fellesdatakatalog.digdir.no Explore and query metrics https://alertmanager.fellesdatakatalog.digdir.no See alerts and silence them https://grafana.fellesdatakatalog.digdir.no Dashboards based on metrics Create metrics endpoint and expose metrics See Metric and label naming for best practices.\nConfigure metrics scraping Use the following pod annotations to configure scraping of metrics:\nannotations: # Enable scraping of metrics. prometheus.io/scrape: \u0026#34;true\u0026#34; # Specifies metrics port. Default: container\u0026#39;s port. prometheus.io/port: \u0026#34;8080\u0026#34; # Specifies metrics path. Default: \u0026#34;/metrics\u0026#34;. prometheus.io/path: \u0026#34;/metrics\u0026#34; If you need more customization, such as scraping interval, look into using a servicemonitor or podmonitor instead.\nCreate alert rules See Alerting Rules for how to configure rules.\nAlerts fire in the #fdk-dev-alerts and #fdk-prod-alerts slack channels.\nPrometheusRule resources in the GitHub repo fdk-infra/infrastructure/base/alerts is used to configure alert rules, and will be automatically synced into Prometheus running in the clusters. Remember to add any new files within the alerts folder to the kustomization.yaml resources list.\n--- apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: \u0026lt;rule name - kebab case\u0026gt; namespace: monitoring labels: release: monitoring-kube-prometheus-stack spec: groups: - name: fdk rules: - alert: \u0026lt;alert name - pascal case\u0026gt; annotations: description: \u0026lt;alert description (shown in slack)\u0026gt; summary: \u0026lt;alert title (shown in slack)\u0026gt; expr: \u0026lt;alert condition (e.g. \u0026#34;up{} == 0\u0026#34;)\u0026gt; for: \u0026lt;time to wait before alerting (e.g. \u0026#34;0s\u0026#34;)\u0026gt; labels: severity: \u0026lt;severity (none|info|warning|error|critical)\u0026gt; dashboard_url: \u0026lt;link to grafana/kibana dashboard, if any\u0026gt; "},{"uri":"/data.norge.no/overordnet-arkitektur/","title":"Overordnet arkitektur","tags":[],"description":"","content":"Her følger en kort oversikt over den overordnede arkitekturen til våre komponenter.\nTOGAF Vi følger en tilpasset versjon av TOGAFs ADM metodikk når vi jobber med arkitektur. Dette betyr at vi tar fram både baseline (Nåsituasjon) og target (Målbilde) modeller, og ut i fra disse utarbeider vi transisjoner og migrasjonsplaner. Transisjons-arkitekturer er å finne i målbildet.\nArchiMate Våre arkitekturmodeller er laget i henhold til Archimate-rammeverket med hjelp av verktøyet Archi. Kildekoden til modellene ligger åpent tilgjengelig for alle på våre Github-side. Hvordan vi har brukt Archi og en enkel oppskrift på hvordan ta dette i bruk vil du finne under de respektive arkitektur-modellene. Det finnes også en en html-versjon av modellene. Linken til disse vil du finne på sidene under.\nAPI-first Vi forsøker å utvikle våre komponenter etter “API”-first metodikken. Vi har derfor valgt å spesifisere våre API etter OpenAPI Specification v 3.\nÅpen kildekode Kildekoden til løsningen ligger åpent tilgjengelig på Github. All kildekode har en åpen lisens Apache License 2.0\n"},{"uri":"/developers_handbook/prepare_your_computer/required_software/","title":"Required software","tags":[],"description":"A list of required software and how to install","content":"In your linux machine you will need to install some software Docker Instructions here\nDocker compose Instructions here\nMaven % sudo apt-get install maven OpenJDK % sudo apt-get install default-jdk This is as of now openjdk-11. If you need java 8, do\n% sudo apt-get install openjdk-8-jdk Kotlin % snap install kotlin Git % sudo apt-get install git Python Python (both 2.7 and 3.) should be preinstalled.\n% python --version Python 2.7.16+ % python3 --version Python 3.7.4 NodeJS It is highly recommended to use the node version manager (nvm). To install nvm, follow instructions.\nThen, to download, compile, and install the latest release of node, do this:\n% nvm install node # \u0026#34;node\u0026#34; is an alias for the latest version "},{"uri":"/developers_handbook/","title":"Developer&#39;s handbook","tags":[],"description":"","content":"The Developer\u0026rsquo;s handbook contains descriptions of our best practices developers will find useful when producing code in our projects. You will find few absolute rules, but you are strongly encouraged to follow practices described here.\n"},{"uri":"/data.norge.no/harvesting/harvest-admin/","title":"Harvest admin","tags":[],"description":"","content":"Backend fdk-harvest-admin\nFrontend fdk-admin-gui\nHarvest Admin is a service that provide functionality to register and list datasource endpoints to be harvested.\n"},{"uri":"/developers_handbook/monitoring/logging/","title":"Logging","tags":[],"description":"","content":"Logs should preferably be in JSON.\n"},{"uri":"/developers_handbook/naming/","title":"Naming","tags":[],"description":"","content":"There are two hard things in Computer Science, and the following is an attempt to ease our pain when naming things.\nWhat Naming convention Example microservice functional description (+service) A backend service repository microservice name a-backend-service api specification microservice name.yaml a-backend-service.yaml groupId no.brreg.servicename no.brreg.abackendservice artifactId service-name a-backend-service package groupId.funtion no.brreg.abackendservice.api|controller|service|repository|model Api classes name-of-resource-in-api-spec+Api SomeResourceApi.java Model classes name-of-resource-in-api-spec+DB SomeResourceDB.java Controller classes name-of-resource-in-api-spec+ApiImpl.java SomeResourceApiImpl.java Service classes name-of-resource-in-api-spec +Service SomeResourceService.java Repository classes name-of-resource-in-api-spec +Repository SomeResourceRepository.java "},{"uri":"/developers_handbook/prepare_your_computer/recommended_software/","title":"Recommended software","tags":[],"description":"A list of recommended software and how to install","content":"The following is a list of useful software that is recommended An IDE ATOM: https://snapcraft.io/atom\n% sudo snap install atom --classic or IntelliJ IDEA Community Edition: https://snapcraft.io/intellij-idea-community\n% sudo snap install intellij-idea-community --classic --edge For Python-hackers, it is highly recommended to install and use virtual-env\nsudo apt-get install python3-venv PyCharm For Python-hackers using PyCharm:\nplugin Add the poetry plugin https://plugins.jetbrains.com/plugin/14307-poetry\ninterpreter Ctrl+Alt+S -\u0026gt; Project -\u0026gt; python interpreter -\u0026gt; cog by dropdown -\u0026gt; Add -\u0026gt; Poetry Environment -\u0026gt; choose python interpreter, i.e. \u0026lsquo;python3.9\u0026rsquo;, from \u0026lsquo;\u0026hellip;/.pyenv/shims/\u0026rsquo; as Base interpreter, and set \u0026lsquo;poetry\u0026rsquo; from \u0026lsquo;\u0026hellip;/.pyenv/shims/\u0026rsquo; as Poetry executable. pytest Ctrl+Alt+S -\u0026gt; Tools -\u0026gt; Python integrated tools -\u0026gt; Testing -\u0026gt; set \u0026lsquo;pytest\u0026rsquo; as Default test runner\nGoogle Cloud SDK sudo snap install google-cloud-sdk --classic You then have to initialize the SDK. Follow instructions here\nkubectl sudo snap install kubectl --classic More [here](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-linux\nYou need to set a default cluster (e.g fdk-dev) for kubectl commands:\ngcloud container clusters get-credentials fdk-dev Enable shell autocompletion for kubectl In short, if you use zsh add the follwing line to the end of your ~/.zshrc file:\nsource \u0026lt;(kubectl completion zsh) Instructions here\n"},{"uri":"/data.norge.no/search/sparql/","title":"SPARQL","tags":[],"description":"","content":"Read more about SPARQL here.\nProduction endpoint: https://sparql.fellesdatakatalog.digdir.no Demo endpoint: https://sparql.demo.fellesdatakatalog.digdir.no Staging endpoint: https://sparql.staging.fellesdatakatalog.digdir.no\nSimple example using the staging endpoint:\ncurl -X POST \u0026#39;https://sparql.staging.fellesdatakatalog.digdir.no/?query=SELECT%20%2A%20WHERE%20%7B%20?sub%20?pred%20?obj%20.%20%7D%20LIMIT%201\u0026#39; Data.norge.no has a simple GUI for SPARQL queries:\nproduction https://data.norge.no/sparql demo https://demo.fellesdatakatalog.digdir.no/sparql staging https://staging.fellesdatakatalog.digdir.no/sparql Query examples List all properties and objects where the subject is this dataset https://staging.fellesdatakatalog.digdir.no/datasets/04edc67b-046c-37a8-9822-29f03d2f1e80:\nPREFIX dcat: \u0026lt;http://www.w3.org/ns/dcat#\u0026gt; PREFIX foaf: \u0026lt;http://xmlns.com/foaf/0.1/\u0026gt; PREFIX dct: \u0026lt;http://purl.org/dc/terms/\u0026gt; SELECT ?property ?object WHERE {​​​​​​ ?dataset a dcat:Dataset . ?record foaf:primaryTopic ?dataset . ?record a dcat:CatalogRecord . ?record dct:identifier \u0026#34;04edc67b-046c-37a8-9822-29f03d2f1e80\u0026#34; . ?dataset ?property ?object . }​​​​​​ List all dataset titles:\nPREFIX dcat: \u0026lt;http://www.w3.org/ns/dcat#\u0026gt; PREFIX dct: \u0026lt;http://purl.org/dc/terms/\u0026gt; SELECT ?title WHERE {​​​​​​​​​​​​​​ ?dataset a dcat:Dataset . ?dataset dct:title ?title . }​​​​​​​​​​​​​​​​​​​​​ "},{"uri":"/developers_handbook/monitoring/grafana/","title":"Grafana","tags":[],"description":"","content":"The repo fdk-grafana-dashboards contains the dashboards that are deployed to FDK Grafana. See the projects readme for more info.\n"},{"uri":"/developers_handbook/testing/","title":"Testing","tags":[],"description":"","content":"On testing microservices and how we do it.\nFrom Clemson \u0026ldquo;Testing Strategies in a Microservice Architecture\u0026rdquo; we have borrowed the following strategies:\nunit integration component contract end-to-end From Clemson \u0026ldquo;Testing Strategies in a Microservice Architecture\u0026rdquo; we have borrowed the following strategies:\nunit (solitary + sociable) automated, included in coverage integration (testing adapters, classes that communicates with the outside world) automated, included in coverage contract (based on specification, \u0026ldquo;service test\u0026rdquo;, the artifact that will be deployed) automated end-to-end (minimal) mostly manual next step\nAs an alternative to end-to-end tests we prefer a technique called semantic monitoring, where we use fake events (synthetic transactions) to ensure that the system is behaving semantically.\nAnother aspect of testing is timing, when to run what tests: References https://martinfowler.com/articles/microservice-testing/ https://samnewman.io/books/building_microservices/ https://martinfowler.com/bliki/SyntheticMonitoring.html "},{"uri":"/developers_handbook/git_workflow/","title":"Git workflow","tags":[],"description":"","content":"There are different popular Git workflows. We consider Gitflow too focused on scheduled releases. Since we try to deploy \u0026ldquo;all the time\u0026rdquo;, we shall adhere to the GitHub Flow:\nAnything in the master branch is deployable To work on something new, create a descriptively named branch off of master (ie: new-oauth2-scopes) Commit to that branch locally and regularly push your work to the same named branch on the server When you need feedback or help, or you think the branch is ready for merging, open a pull request After someone else has reviewed and signed off on the feature, you can merge it into master Once it is merged and pushed to \u0026lsquo;master\u0026rsquo;, you can and should deploy immediately Tips and tricks Nice introductction to GitHub flow here: https://docs.github.com/en/get-started/quickstart/github-flow "},{"uri":"/developers_handbook/code_quality/","title":"Code quality","tags":[],"description":"","content":"We use SonarCloud to monitor code quality, including test coverage.\nFirst and foremost code quality analysis is a tool for code review. In the following pages you will find a brief discussion on what we consider important aspects of quality when doing a review.\n"},{"uri":"/data.norge.no/metadatakvalitet/","title":"Metadatakvalitet","tags":[],"description":"","content":" d a t a s e t K - a e f v k d e a a n t t a - s p e u t b - l e i v s e h n a e t s r s s m e n t a m t R q o a a r b - b h d i a a t r t M v a Q e s s e t t P s - o e s v t e g n r t e s s d a p t r a o u s p r e e l s t r - c - t c o h y h r a - e i r c c n v h k g e e e - s c r a t k p e e i r r m q a - s e c v o e r n i t n s g - s e r v i c e dataset-event-publisher lytter etter RabbitMQ harvest meldinger fra dataset-harvester. Den henter så datasett-grafer via apiet til dataset-harvester, og produserer én event på dataset-events topicen for hvert datasett dataset-harvester har oppdatert. assmentator konsumerer hver høstet graf fra dataset-events, legger til hasAssessment properties på både dataset- og hver distribution-node, og produserer videre til mqa-dataset-events. property-checker og url-checker konsumerer begge datasett grafer fra mqa-dataset-events og produserer assessment grafer til mqa-events. Disse assessment grafene konsumeres av scoring-service, og sammenslås basert på fdk_id og event timestamp. scoring-service lagrer/oppdaterer assessment grafer i Postgres via scoring-api, samt totalscore for hver dimensjon for å kunne gjøre raske aggregerte spørringer.\n"},{"uri":"/developers_handbook/monitoring/","title":"Monitoring","tags":[],"description":"","content":"Monitoring is critical to a microservice architecture. In the following pages we describe how we instrument our services in order to be properly monitored.\nWe use Prometheus and Grafana to monitor our services. They are configured and deployed from the fdk-infra repo. The Grafana dashboards themselves are located in fdk-grafana-dashboards.\n"},{"uri":"/developers_handbook/operations_and_status_codes/","title":"Operations and status codes","tags":[],"description":"","content":"The following is guidelines prescribing what HTTP method to use for common \u0026ldquo;business functions\u0026rdquo; and what status codes to return for the happy case. If a header is to be returned, an example is given.\nFunction HTTP Method Status Code Header Comment List GET /foos/ 200 OK Collection in response. 200 Ok even if empty collection Create POST /foos/ 201 Created Location: /foos/1234 No body in response Get GET /foos/1234 200 OK Update PUT /foos/1234 204 No Content No body in response Update (patch) PATCH /foos/1234 200 OK The updated resource in response body Delete DELETE /foos/1234 204 No Content No body in response References Method definitions Status Code Definitions\nJSON Patch\n"},{"uri":"/developers_handbook/prepare_your_computer/","title":"Prepare your computer","tags":[],"description":"","content":"The following is what is intended as useful hints and howtos when you set up your computer for working with our projects. The following tips are in no way requirements, but should help you to get started painlessly.\nLinux\nRequired software\nRecommended software\n"},{"uri":"/data.norge.no/search/","title":"Search","tags":[],"description":"","content":"There are 3 different ways to search for data in Data.norge.no, each is based on a different technology.\nSearch The main search service in Data.norge.no is based on ElasticSearch. This service searches the text in a limited selection of fields, and has filters and other advanced functionality that helps users navigate.\nSee this page for more info\nLLM The LLM search service uses a \u0026lsquo;Large Language Model\u0026rsquo; (LLM) that is built from the same data used in the main search, but will accept naturally worded queries where the other search demands exact wording of titles or keywords.\nSee this page for more info\nSPARQL The SPARQL search service is based on the RDF query language SPARQL, and is our most advanced and powerful search. A user will have to create queries that follows the correct syntax, but will be able to search all harvested data points, not just pre-selected fields.\nSee this page for more info\n"},{"uri":"/data.norge.no/harvesting/","title":"Harvesting","tags":[],"description":"","content":"\nThe harvest solution consists of 3 parts:\nHarvest - The resources are downloaded from several sources, split into single resource-graphs and given an id Reason - Each resource graph is enriched with relevant data Parse - Each resource graph is converted to JSON The finished JSON-representation of each resource is then picked up by both the backend for the search page, fdk-search-service, and the backend for the details page, fdk-resource-service. Changes in the resources are available in data.norge when these two services have been updated.\nThe harvest process can be initiated by two services:\nHarvest scheduler - Initiates harvest of all sources from predefined schedules. Harvest admin - This is where sources are registered for harvest, each source has a button to initiate harvest of that specific source. The communication between the relevant services is handled by a combination of RabbitMQ and Apache Kafka. The harvests are triggered by messages published in RabbitMQ, and the harvesters will publish harvest reports for each source in RabbitMQ when they are done. These reports contain information about each resource with changes and each resource that has been removed from the source since the last harvest. These reports are picked up by different versions of fdk-kafka-event-publisher, and will produce events in Kafka for each changed resource. Reasoning consumes events about changed resources and produces new events with the enriched graphs. Parsing consumes events about reasoned resources and produces new events with a JSON version of the resource. The events about parsed resources are consumed by fdk-search-service and fdk-resource-service and the harvest process is finished.\nParts of FDK not strictly part of the harvest process that are also dependent of the kafka events produced by the process:\nfdk-sparql-service That listens for reasoned and removed messages to maintain updated graphs available for sparql queries MQA That listens for DATASET_HARVESTED to produce an assessment of the harvested datasets Detailed schema of the harvest process "},{"uri":"/developers_handbook/python/","title":"Python","tags":[],"description":"","content":" We use pyenv to manage python versions on our systems. We use pipx for installing cli apps. We use poetry, nox and nox-poetry to manage dependencies and automate our testing. This stack is also used in our CI workflows. Even though all of the tools are very well documented, there are things that you need to do once in a while that can be hard to remember.\nThe following is therefore a list of usefull things to know which is not easily found in the documentation.\nUpdate pyenv in order to install newer python:\n% pyenv update List all packages and version-information:\n% pipx runpip nox list Uninstall a package previously injected:\n% pipx runpip nox uninstall nox-poetry References Method definitions Status Code Definitions\nJSON Patch\n"},{"uri":"/developers_handbook/rust/","title":"Rust","tags":[],"description":"","content":"\nTo install Rust, refer to the Installing Rust section within the Getting Started page.\nThe Rust Book is a great place to start learning Rust.\nIDE recommendations\nVisual Studio Code with the rust-analyzer extension\nRustRover\n"},{"uri":"/developers_handbook/kafka/","title":"Kafka","tags":[],"description":"","content":"Read messages from Kafka Number of partitions and their message count Substitute dataset-events with the desired topic.\nkubectl exec -it kafka-1-0 -- /bin/kafka-run-class kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic dataset-events You may omit --topic to list all topics, but should then ignore __consumer and _schemas.\nkubectl exec -it kafka-1-0 -- /bin/kafka-run-class kafka.tools.GetOffsetShell --broker-list localhost:9092 | grep -v __consumer | grep -v _schemas Example output dataset-events:0:47472 dataset-events:1:49949 dataset-events:2:49361 dataset-events:3:50122 Read last n messages from partition in topic Substitute topic, partition and offset parameters based on output from command above. Remember that all messages with the same key (often some sort of ID) is put on the same partition.\nkubectl exec -it schema-registry-1-0 -- kafka-avro-console-consumer --bootstrap-server kafka-1:9092 --property schema.registry.url=http://localhost:8081 --topic dataset-events --partition 0 --offset 47470 You may redirect the command output to a file and then use jq to get a more readable output.\n# Pipe output into file \u0026#39;kafka-output\u0026#39; while also seeing output in terminal kubectl exec ... | tee kafka-output # Show whole messages cat kafka-output | grep \u0026#34;{\u0026#34; | jq # Show selected fields in messages cat kafka-output | grep \u0026#34;{\u0026#34; | jq \u0026#34;{type,fdkId,timestamp}\u0026#34; "},{"uri":"/developers_handbook/kubernetes/","title":"Kubernetes","tags":[],"description":"","content":"Kubectl alias (replace \u0026lt;project\u0026gt;, \u0026lt;project\u0026gt;, \u0026lt;region\u0026gt;, and \u0026lt;namespace\u0026gt;):\nalias kprod=\u0026#39;gcloud config set project \u0026lt;project\u0026gt;; gcloud container clusters get-credentials \u0026lt;project\u0026gt; --region \u0026lt;region\u0026gt;; kubectl config set-context --current --namespace=\u0026lt;namespace\u0026gt;\u0026#39; Delete all Trivy vulnerability reports:\nkubectl get vulnerabilityreports.aquasecurity.github.io | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl delete vulnerabilityreports.aquasecurity.github.io Search for specific vulnerability (replace \u0026lt;vuln-to-search-for\u0026gt;):\nkubectl get vulnerabilityreports.aquasecurity.github.io -o json | jq -c .items[].report.vulnerabilities[] | grep \u0026lt;vuln-to-search-for\u0026gt; | jq -rc \u0026#34;[.resource,.installedVersion,.title]\u0026#34; | sort | uniq Set node label (replace \u0026lt;nodename\u0026gt; and \u0026lt;label\u0026gt; (eg app=static-rdf)):\nkubectl get nodes --show-labels kubectl label nodes \u0026lt;nodename\u0026gt; \u0026lt;label\u0026gt; "},{"uri":"/data.norge.no/statistikk/","title":"Statistikk","tags":[],"description":"","content":" Oktober 2018 April 2019 Mai 2019 Juni 2019 Juli 2019 August 2019 September 2019 Oktober 2019 November 2019 "},{"uri":"/_header/","title":"","tags":[],"description":"","content":"\n"},{"uri":"/developers_handbook/go/","title":"","tags":[],"description":"","content":"Go\nTo install Go, refer to the Download and install section within the Go User Manual. The Getting started section is a great place to start learning Go. IDE recommendations\nVisual Studio Code with the Go extension GoLand "},{"uri":"/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"/","title":"Data.norge.no documentation","tags":[],"description":"","content":"Data.norge.no documentation Here you can find more information and documentation about Data.norge.no.\nIf you find something that is wrong or not described good enough in our documentation:\nsend an email to fellesdatakatalog@digdir.no create an issue in Github Informasjonsforvaltning/docs create a pull request in Github Informasjonsforvaltning/docs If you find something that is wrong or missing in Data.norge.no:\nsend an email to fellesdatakatalog@digdir.no create an issue in Github Informasjonsforvaltning/fdk-issue-tracker. "},{"uri":"/tags/","title":"Tags","tags":[],"description":"","content":""}]